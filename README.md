# Fraud Detection for E-commerce and Bank Transactions

A comprehensive machine learning system for detecting fraudulent transactions in e-commerce and credit card data. This project implements industry-standard practices for data analysis, feature engineering, model training, and evaluation.

## ğŸ¯ Project Overview

This project analyzes two datasets:
- **E-commerce Fraud Data**: Transaction data with user behavior and geolocation features
- **Credit Card Transactions**: Anonymized credit card transaction data with PCA-transformed features

### Key Features

- âœ… Comprehensive exploratory data analysis (EDA)
- âœ… Advanced feature engineering (time-based, frequency, velocity features)
- âœ… Geolocation integration with IP-to-country mapping
- âœ… Class imbalance handling (SMOTE, undersampling)
- âœ… Multiple ML models (Logistic Regression, Random Forest, XGBoost, LightGBM)
- âœ… Stratified K-fold cross-validation
- âœ… Model interpretability with SHAP
- âœ… Comprehensive testing suite
- âœ… CI/CD with GitHub Actions

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip package manager
- Virtual environment (recommended)

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/Eden-kassie/Fraud-Detection.git
cd Fraud-Detection
```

### 2. Set Up Virtual Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Windows:
.venv\Scripts\activate
# On macOS/Linux:
source .venv/bin/activate
```

### 3. Install Dependencies

```bash
# Install package in editable mode
pip install -e .

# Or install from requirements.txt
pip install -r requirements.txt
```

### 4. Prepare Data

Place your datasets in the `data/raw/` directory:
- `Fraud_Data.csv`
- `IpAddress_to_Country.csv`
- `creditcard.csv`

## ğŸ“ Project Structure

```
fraud-detection/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ unittests.yml          # CI/CD pipeline
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json              # VS Code configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Original datasets (gitignored)
â”‚   â””â”€â”€ processed/                 # Processed datasets (gitignored)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda-fraud-data.ipynb       # EDA for e-commerce data
â”‚   â”œâ”€â”€ eda-creditcard.ipynb       # EDA for credit card data
â”‚   â”œâ”€â”€ feature-engineering.ipynb  # Feature creation
â”‚   â”œâ”€â”€ modeling.ipynb             # Model training & evaluation
â”‚   â”œâ”€â”€ shap-explainability.ipynb  # Model interpretation
â”‚   â””â”€â”€ README.md                  # Notebook documentation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                      # Data loading and preprocessing
â”‚   â”œâ”€â”€ features/                  # Feature engineering
â”‚   â”œâ”€â”€ models/                    # Model training and evaluation
â”‚   â”œâ”€â”€ visualization/             # Plotting utilities
â”‚   â””â”€â”€ utils/                     # Helper functions
â”œâ”€â”€ tests/                         # Unit tests
â”œâ”€â”€ scripts/                       # Utility scripts
â”œâ”€â”€ models/                        # Saved model artifacts (gitignored)
â”œâ”€â”€ requirements.txt               # Project dependencies
â”œâ”€â”€ setup.py                       # Package configuration
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â””â”€â”€ README.md                      # This file
```

## ğŸ“Š Usage

### Running Notebooks

Execute notebooks in the following order:

1. **Exploratory Data Analysis**
   ```bash
   jupyter notebook notebooks/eda-fraud-data.ipynb
   jupyter notebook notebooks/eda-creditcard.ipynb
   ```

2. **Feature Engineering**
   ```bash
   jupyter notebook notebooks/feature-engineering.ipynb
   ```

3. **Model Training**
   ```bash
   jupyter notebook notebooks/modeling.ipynb
   ```

4. **Model Interpretation**
   ```bash
   jupyter notebook notebooks/shap-explainability.ipynb
   ```

### Using the Python Package

```python
from src.data.loading import load_fraud_data, load_creditcard_data
from src.features.engineering import create_all_features
from src.models.baseline import LogisticRegressionBaseline
from src.models.ensemble import XGBoostModel

# Load data
fraud_df = load_fraud_data()

# Create features
fraud_df = create_all_features(fraud_df)

# Train model
model = XGBoostModel()
model.train(X_train, y_train)

# Evaluate
metrics = model.evaluate(X_test, y_test)
print(metrics)
```

### Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=html

# Run specific test file
pytest tests/test_features.py -v
```

### Code Quality

```bash
# Format code
black src/ tests/

# Check linting
flake8 src/ tests/

# Type checking
mypy src/
```

## ğŸ¯ Model Performance

### E-commerce Fraud Detection
| Model | AUC-PR | F1-Score | Precision | Recall |
|-------|--------|----------|-----------|--------|
| Logistic Regression | TBD | TBD | TBD | TBD |
| Random Forest | TBD | TBD | TBD | TBD |
| XGBoost | TBD | TBD | TBD | TBD |
| LightGBM | TBD | TBD | TBD | TBD |

### Credit Card Fraud Detection
| Model | AUC-PR | F1-Score | Precision | Recall |
|-------|--------|----------|-----------|--------|
| Logistic Regression | TBD | TBD | TBD | TBD |
| Random Forest | TBD | TBD | TBD | TBD |
| XGBoost | TBD | TBD | TBD | TBD |
| LightGBM | TBD | TBD | TBD | TBD |

*Note: Metrics will be updated after model training*

## ğŸ”¬ Methodology

### Data Preprocessing
1. Handle missing values with appropriate imputation
2. Remove duplicate records
3. Correct data types
4. Merge geolocation data using IP address ranges

### Feature Engineering
- **Time-based features**: hour of day, day of week, time since signup
- **Transaction features**: frequency, velocity, amount statistics
- **Geolocation features**: country-based risk scores

### Class Imbalance Handling
- SMOTE (Synthetic Minority Over-sampling Technique)
- Random undersampling
- Stratified sampling for train-test split

### Model Training
- Baseline: Logistic Regression
- Ensemble: Random Forest, XGBoost, LightGBM
- Hyperparameter tuning with cross-validation
- Stratified 5-fold cross-validation

### Evaluation Metrics
- **AUC-PR**: Area under Precision-Recall curve (primary metric for imbalanced data)
- **F1-Score**: Harmonic mean of precision and recall
- **Confusion Matrix**: Detailed classification results
- **Cross-validation**: Mean and standard deviation across folds

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guide
- Use Black for code formatting (line length: 100)
- Write unit tests for new features
- Update documentation as needed
- Ensure all tests pass before submitting PR

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¥ Authors

- Eden Moges - Initial work

## ğŸ™ Acknowledgments

- Dataset sources
- Scikit-learn, XGBoost, LightGBM communities
- SHAP for model interpretability

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

